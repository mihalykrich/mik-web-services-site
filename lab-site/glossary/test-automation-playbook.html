<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Test automation playbook | MIK Web Services</title>
    <meta name="description" content="From device drivers to report pipelines: patterns for robust lab automation.">
    <link rel="canonical" href="https://mik-webservices.co.uk/glossary/test-automation-playbook.html">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Test automation playbook">
    <meta property="og:description" content="Patterns for robust lab automation from drivers to reporting.">
    <meta property="og:image" content="../assets/img/og-cover.svg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Test automation playbook">
    <meta name="twitter:description" content="Patterns for robust lab automation from drivers to reporting.">
    <meta name="twitter:image" content="../assets/img/og-cover.svg">
    <link rel="stylesheet" href="../assets/css/main.css">
  </head>
  <body>
    <main class="section">
      <div class="container">
        <p><a class="back-link" href="../index.html#glossary">← Back to Glossary</a></p>
        <h1>Test automation playbook</h1>
        <p>Build resilient pipelines: device abstraction, state machines, structured logging, and automated report generation.</p>
        <h2>1) Architecture</h2>
        <h3>1.1 Device layer — unified drivers + simulators</h3>
        <p><strong>Goal:</strong> One interface for real hardware and CI simulators.</p>
        <p><strong>Pattern:</strong> <code>IDevice</code> interface → concrete <code>UsbPump</code>, <code>TcpSensor</code> + <code>SimPump</code>, <code>SimSensor</code>.</p>
        <ul>
          <li>Runtime selection: env flag <code>DEVICE_BACKEND=sim|real</code>.</li>
          <li>Dependency injection: pass device handles into controllers (no globals).</li>
        </ul>
        <pre><code># devices/base.py
class Pump:
    async def prime(self, volume_ml: float) -> None: ...
    async def dispense(self, volume_ml: float) -> None: ...
    async def status(self) -> dict: ...

# devices/usb_pump.py / devices/sim_pump.py implement Pump
</code></pre>
        <p>CI: default to <code>Sim*</code> backends; simulate latency, jitter, and faults.</p>

        <h3>1.2 Controller layer — state machines with timeouts & retries</h3>
        <p>Each procedure = explicit FSM (states, transitions, guards). Built‑ins: per‑step timeout, bounded retries, exponential backoff, and abort on hazard.</p>
        <pre><code># controllers/procedure.py
from enum import Enum, auto

class S(Enum): IDLE=auto(); PRIME=auto(); DISPENSE=auto(); VERIFY=auto(); DONE=auto(); FAIL=auto()

async def run(ctx):
    s=S.IDLE
    while True:
        if s is S.IDLE:
            s=S.PRIME
        elif s is S.PRIME:
            await with_retry(ctx.pump.prime, volume_ml=2.0, timeout=5, retries=3)
            s=S.DISPENSE
        elif s is S.DISPENSE:
            await with_retry(ctx.pump.dispense, volume_ml=10.0, timeout=10, retries=2)
            s=S.VERIFY
        elif s is S.VERIFY:
            ok = await verify_volume(ctx)
            s = S.DONE if ok else S.FAIL
        elif s in (S.DONE, S.FAIL):
            return s
</code></pre>

        <h3>1.3 Data layer — schemas & versioning</h3>
        <p>Single source of truth for samples, configs, results. Version every schema; keep migrations in repo.</p>
        <pre><code># schemas/config.schema.json (v3)
$schema: "https://json-schema.org/draft/2020-12/schema"
title: "RunConfig"
version: 3
type: object
properties:
  run_id: {type: string}
  sample_id: {type: string}
  target_volume_ml: {type: number}
  device_profile: {type: string, enum: [sim, real]}
required: [run_id, sample_id, target_volume_ml]
</code></pre>
        <pre><code>runs/
  2025-11-03T10-12-22Z_run-8421/
    config.v3.json
    results.v2.json
    artifacts/
      logs.ndjson
      traces/
      attachments/
</code></pre>

        <h2>2) Reliability</h2>
        <h3>2.1 Idempotent steps & checkpoints</h3>
        <p>Idempotency key per step (e.g., <code>run_id:step_name:index</code>) to avoid double‑actions. Checkpointing: write <code>state.json</code> after each successful step (atomic rename).</p>
        <pre><code>def checkpoint(run_dir, step_name, payload):
    tmp = run_dir/".state.json.tmp"
    json.dump({"step": step_name, **payload}, tmp.open("w"))
    tmp.replace(run_dir/"state.json")
</code></pre>
        <p>Resume logic: on start, read last checkpoint and jump to the next state.</p>
        <h3>2.2 Structured logs + metrics + alerts</h3>
        <p>Logs: newline‑delimited JSON. Always include <code>run_id</code>, <code>step</code>, <code>device</code>, <code>ts</code>, <code>level</code>.</p>
        <pre><code>{"ts":"2025-11-03T10:12:28Z","level":"INFO","run_id":"8421","step":"DISPENSE","ml":10.0,"lat_ms":842}
</code></pre>
        <p>Metrics: counters, gauges, histograms. Alerts on SLO breaches and error rate spikes.</p>
        <pre><code>- alert: HighFailureRate
  expr: sum(rate(procedure_fail_total[5m])) / sum(rate(procedure_start_total[5m])) > 0.05
  for: 10m
  labels: {severity: page}
</code></pre>
        <h3>2.3 Golden tests & fuzzing</h3>
        <pre><code># Golden
expected = Path("goldens/result_v2.json").read_text()
assert normalize(actual_json) == normalize(expected)

# Fuzz (hypothesis)
from hypothesis import given, strategies as st
@given(st.text(min_size=0, max_size=1024))
def test_parser_never_crashes(s):
    parse_csv_maybe(s)  # should not raise
</code></pre>

        <h2>3) Reporting & Distribution</h2>
        <h3>3.1 Generate signed PDFs/CSV</h3>
        <p>Render PDF from HTML with plots; CSV as machine‑readable results. Attach a detached signature & manifest.</p>
        <pre><code>SHA256  reports/run-8421.pdf  9a1e...
SHA256  reports/run-8421.csv  4b7c...
</code></pre>
        <h3>3.2 Automate distribution & archival</h3>
        <p>Bundle <code>trace.tgz</code> with config, logs, results, reports, manifest, and signature. Upload to object storage; notify Slack/Email. Apply lifecycle rules (e.g., 180 days).</p>

        <h2>4) CI/CD wiring</h2>
        <h3>4.1 GitHub Actions (example)</h3>
        <pre><code>name: pipeline
on: [push, workflow_dispatch]
jobs:
  test:
    runs-on: ubuntu-latest
    env: { DEVICE_BACKEND: sim }
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.12' }
      - run: pip install -r requirements.txt
      - name: Unit + golden + fuzz smoke
        run: |
          pytest -q tests/unit
          pytest -q tests/golden
          pytest -q -k "fuzz and smoke"
      - name: Build report artifact
        run: python tools/build_report.py --run-id ${{ github.run_id }}
      - uses: actions/upload-artifact@v4
        with:
          name: trace-bundle
          path: runs/**/artifacts/*
</code></pre>
        <h3>4.2 Gates</h3>
        <ul>
          <li>✅ All device simulators pass.</li>
          <li>✅ Golden diffs clean (or explicitly updated in PR).</li>
          <li>✅ Coverage ≥ threshold on controllers & parsers.</li>
          <li>✅ Lint + typecheck green.</li>
        </ul>

        <h2>5) Minimal templates (drop‑in)</h2>
        <h3>Structured logging helper</h3>
        <pre><code>import json, sys, time
def log(event, **kw):
    kw.setdefault("ts", time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()))
    sys.stdout.write(json.dumps({"event": event, **kw}) + "\n")
</code></pre>
        <h3>Retry with timeout</h3>
        <pre><code>import asyncio
async def with_retry(fn, *, timeout, retries, backoff=0.5, **kw):
    for i in range(retries + 1):
        try:
            return await asyncio.wait_for(fn(**kw), timeout)
        except Exception:
            if i == retries: raise
            await asyncio.sleep(backoff * (2 ** i))
</code></pre>
        <h3>Report builder (skeleton)</h3>
        <pre><code># tools/build_report.py
def build(run_dir):
    data = json.load(open(run_dir/"results.v2.json"))
    html = render_html(data)       # your template
    pdf_path = pdf_from_html(html) # your engine
    csv_path = write_csv(data)
    write_manifest_and_sign([pdf_path, csv_path])
</code></pre>

        <h2>6) Checklists</h2>
        <h3>Device layer</h3>
        <ul>
          <li>Real & simulated drivers implement same interface</li>
          <li>Fault injection knobs (latency, drop, corrupt)</li>
          <li>Hardware feature flags in config schema</li>
        </ul>
        <h3>Controllers</h3>
        <ul>
          <li>Explicit FSM per procedure</li>
          <li>Timeouts + retries + backoff per step</li>
          <li>Idempotency keys + checkpoints</li>
        </ul>
        <h3>Data & logs</h3>
        <ul>
          <li>Versioned schemas + migrations</li>
          <li>NDJSON logs with run_id, step</li>
          <li>Metrics: latency histograms, error counters</li>
        </ul>
        <h3>Testing</h3>
        <ul>
          <li>Golden tests for critical paths</li>
          <li>Fuzz tests for parsers</li>
          <li>CI default to simulators</li>
        </ul>
        <h3>Reporting</h3>
        <ul>
          <li>PDF + CSV rendered and signed</li>
          <li>Trace bundle packaged, uploaded, retained</li>
          <li>Notifications sent with links</li>
        </ul>

        <h2>7) Example “happy path” flow</h2>
        <ol>
          <li>Receive RunConfig v3 → validate schema.</li>
          <li>Spin up Sim* or real devices via DI.</li>
          <li>Execute controller FSM with checkpoints.</li>
          <li>Emit NDJSON logs + metrics.</li>
          <li>Persist results.v2.json.</li>
          <li>Generate PDF/CSV + signatures.</li>
          <li>Bundle traces → upload → notify → archive with retention policy.</li>
        </ol>
        <p>Want a tailored playbook for your lab? <a class="back-link" href="../index.html#contact">Request a workshop</a>.</p>
      </div>
    </main>
    <script type="application/ld+json">
    {
      "@context":"https://schema.org",
      "@type":"Article",
      "headline":"Test automation playbook",
      "author":{"@type":"Organization","name":"MIK Web Services"},
      "publisher":{"@type":"Organization","name":"MIK Web Services","logo":{"@type":"ImageObject","url":"https://mik-webservices.co.uk/assets/img/logo.svg"}},
      "datePublished":"2025-01-01",
      "dateModified":"2025-01-01",
      "image":"https://mik-webservices.co.uk/assets/img/og-cover.svg",
      "mainEntityOfPage":"https://mik-webservices.co.uk/glossary/test-automation-playbook.html"
    }
    </script>
    <script type="application/ld+json">
    {
      "@context":"https://schema.org",
      "@type":"BreadcrumbList",
      "itemListElement":[
        {"@type":"ListItem","position":1,"name":"Home","item":"https://mik-webservices.co.uk/#"},
        {"@type":"ListItem","position":2,"name":"Glossary","item":"https://mik-webservices.co.uk/#glossary"},
        {"@type":"ListItem","position":3,"name":"Test automation playbook","item":"https://mik-webservices.co.uk/glossary/test-automation-playbook.html"}
      ]
    }
    </script>
  </body>
  </html>


